{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interested Root Codes\n",
    "root_codes = [11, 17, 12, 13, 16, 15, 14]\n",
    "root_code_dict = {17: \"Coerce\",\n",
    "                  16: \"Reduce relation\",\n",
    "                  15: \"Exhibit force posture\",\n",
    "                  14: \"Protest\",\n",
    "                  13: \"Threaten\",\n",
    "                  12: \"Reject\",\n",
    "                  11: \"Disapprove\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "bln_nyt = pd.read_csv(\"PhoenixBLN-NYT_1980-2018.csv\")\n",
    "bln_swb = pd.read_csv(\"PhoenixBLN-SWB_1979-2019.csv\")\n",
    "fbis = pd.read_csv(\"PhoenixFBIS_1995-2004.csv\")\n",
    "nyt = pd.read_csv(\"PhoenixNYT_1945-2005.csv\")\n",
    "wsj = pd.read_csv(\"PhoenixWSJ_1945-2006.csv\")\n",
    "\n",
    "# Correct bln_nyt column names\n",
    "bln_nyt.columns = bln_swb.columns\n",
    "\n",
    "# Add dataset information as column\n",
    "bln_nyt[\"dataset\"] = \"bln_nyt\"\n",
    "bln_swb[\"dataset\"] = \"bln_swb\"\n",
    "fbis[\"dataset\"] = \"fbis\"\n",
    "nyt[\"dataset\"] = \"nyt\"\n",
    "wsj[\"dataset\"] = \"wsj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets list\n",
    "datasets = [bln_nyt, bln_swb, fbis, nyt, wsj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter datasets based on interested root_codes\n",
    "bln_nyt = bln_nyt[bln_nyt[\"root_code\"].isin(root_codes)]\n",
    "bln_swb = bln_swb[bln_swb[\"root_code\"].isin(root_codes)]\n",
    "fbis = fbis[fbis[\"root_code\"].isin(root_codes)]\n",
    "nyt = nyt[nyt[\"root_code\"].isin(root_codes)]\n",
    "wsj = wsj[wsj[\"root_code\"].isin(root_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Dataset Sizes***\n",
      "Dataset: bln_nyt  Shape: (1416962, 27)\n",
      "Dataset: bln_swb  Shape: (4255929, 27)\n",
      "Dataset: fbis  Shape: (817781, 26)\n",
      "Dataset: nyt  Shape: (1092211, 26)\n",
      "Dataset: wsj  Shape: (589886, 27)\n"
     ]
    }
   ],
   "source": [
    "print(\"***Dataset Sizes***\")\n",
    "# Checkout dataset sizes\n",
    "for a, i in enumerate(datasets):\n",
    "    print(\"Dataset:\", i[\"dataset\"].values[0], \" Shape:\", i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size: (8172769, 27)\n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets\n",
    "combined = pd.concat(datasets, axis=0)\n",
    "\n",
    "# Fix index order\n",
    "combined.index = range(len(combined))\n",
    "\n",
    "# Shape of combined dataset\n",
    "print(\"Combined dataset size:\", combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size after dropping null source_root: (4979422, 27)\n"
     ]
    }
   ],
   "source": [
    "# Drop null rows based on source_root column\n",
    "combined.dropna(subset=[\"source_root\"], inplace=True)\n",
    "\n",
    "# Shape of combined dataset\n",
    "print(\"Combined dataset size after dropping null source_root:\", combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size after filtering interested root_codes: (669935, 27)\n"
     ]
    }
   ],
   "source": [
    "# Filter combined dataset based on interested root_codes\n",
    "combined = combined[combined[\"root_code\"].isin(root_codes)]\n",
    "\n",
    "# Shape of combined dataset\n",
    "print(\"Combined dataset size after filtering interested root_codes:\", combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert root_code values to actual texts\n",
    "combined[\"root_code\"] = combined[\"root_code\"].map(root_code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for duplicate check\n",
    "required_columns = [\"year\", \"month\", \"day\", \"source\", \"source_root\", \"target\", \"target_root\", \"code\", \"root_code\", \"goldstein\"]\n",
    "\n",
    "# Find out duplicate row indexes based on their dataset value counts for each duplicate pair, keep dataset rows with higher value count \n",
    "duplicate_check = combined[combined.duplicated(subset=required_columns, keep=False)].sort_values(required_columns)\n",
    "drop_list = list()\n",
    "for (_, _, _, _, _, _, _, _, _, _,), group in duplicate_check.groupby(required_columns):\n",
    "    if group[\"dataset\"].nunique() > 1:\n",
    "        source_counts = group['dataset'].value_counts()\n",
    "        most_frequent_source = source_counts.idxmax()\n",
    "        other_indexes = group[group['dataset'] != most_frequent_source].index.tolist()\n",
    "        drop_list = other_indexes + drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecessary dataset column\n",
    "del combined[\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size after dropping duplicates: (648311, 26)\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate value indexes\n",
    "combined = combined.drop(index=drop_list)\n",
    "\n",
    "# Shape of combined dataset\n",
    "print(\"Combined dataset size after dropping duplicates:\", combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RUS source_root\n",
    "map_source_root = {\"FRA\": \"FRA\",\n",
    "                   \"GBR\": \"GBR\",\n",
    "                   \"USA\": \"USA\",\n",
    "                   \"RUS\": \"RUS\",\n",
    "                   \"SUN\": \"RUS\",\n",
    "                   \"POL\": \"RUS\",\n",
    "                   \"CSK\": \"RUS\",\n",
    "                   \"HUN\": \"RUS\",\n",
    "                   \"DDR\": \"RUS\",\n",
    "                   \"ROU\": \"RUS\",\n",
    "                   \"BGR\": \"RUS\"}\n",
    "\n",
    "# Apply map_source_root\n",
    "combined[\"source_root\"] = combined[\"source_root\"].replace(map_source_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size after dropping duplicates: (495440, 26)\n"
     ]
    }
   ],
   "source": [
    "combined = combined[combined['source_root'] != combined['target_root']]\n",
    "\n",
    "# Shape of combined dataset\n",
    "print(\"Combined dataset size after dropping duplicates:\", combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset \n",
    "combined.to_csv(\"combined_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to their periods with filtering\n",
    "period_1 = combined[(combined['year'] >= 1946) & (combined['year'] <= 1999) & (combined[\"source_root\"].isin([\"FRA\", \"GBR\", \"USA\", \"RUS\", \"SUN\", \"POL\", \"CSK\", \"HUN\", \"DDR\", \"ROU\", \"BGR\"]))]\n",
    "period_2 = combined[(combined['year'] >= 2000) & (combined['year'] <= 2022) & (combined[\"source_root\"].isin([\"FRA\", \"GBR\", \"USA\", \"RUS\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_3512\\747304399.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  period_1[\"root_code\"] = period_1[\"root_code\"].map(reverse_root_code_dict)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_3512\\747304399.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  period_2[\"root_code\"] = period_2[\"root_code\"].map(reverse_root_code_dict)\n"
     ]
    }
   ],
   "source": [
    "# Create reverse map dict based on root_code\n",
    "reverse_root_code_dict = {v: k for k, v in root_code_dict.items()}\n",
    "\n",
    "# Convert root_code from text to codes\n",
    "period_1[\"root_code\"] = period_1[\"root_code\"].map(reverse_root_code_dict)\n",
    "period_2[\"root_code\"] = period_2[\"root_code\"].map(reverse_root_code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert source_root to dataframe index\n",
    "p1 = period_1.set_index(\"source_root\").sort_values([\"source_root\", \"year\", \"month\", \"day\"])[[\"target_root\", \"target\", \"root_code\", \"code\", \"year\", \"month\", \"day\"]]\n",
    "p2 = period_2.set_index(\"source_root\").sort_values([\"source_root\", \"year\", \"month\", \"day\"])[[\"target_root\", \"target\", \"root_code\", \"code\", \"year\", \"month\", \"day\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.to_excel(\"phoenix list 46-99.xlsx\")\n",
    "p2.to_excel(\"phoenix list 2000-recent.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
